{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upgrade requests to version >= 2.18.0 due to google-cloud-storage having to low dependency boundary. After upgrading you may need to restart the interpreter by shutting down session and open up the notebook again. The reason is that the old requests package version may be \"cached\" in sys.module causing errors in google-cloud-storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.18.4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check that requests version is higher than 2.18.0\n",
    "import requests\n",
    "requests.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "#!pip install 'requests>= 2.18.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.storage import Client as StorageClient\n",
    "\n",
    "from pandas import read_csv\n",
    "\n",
    "from apiclient.http import MediaFileUpload\n",
    "\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from googleapiclient import discovery\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "\n",
    "analytics = discovery.build('analytics', 'v3', credentials=credentials)\n",
    "\n",
    "#Execute a asyncronous query job and store result in temporary table. Thereafter extract table to google cloud storage\n",
    "def bq_to_cs(_query, _project, _blob, _bucket, _destination_format='CSV', _print_header=True, _write_disposition='WRITE_TRUNCATE', _legacy_sql=False):\n",
    "    client = bigquery.Client(project=_project)        \n",
    "    query_job = client.query(_query)\n",
    "    query_job.result()\n",
    "    \n",
    "    table_ref = query_job.destination\n",
    "\n",
    "    destination_uri = 'gs://{}/{}'.format(_bucket, _blob)\n",
    "    extract_job = client.extract_table(\n",
    "        table_ref, destination_uri)  # API request\n",
    "\n",
    "    extract_job.result(timeout=100)  # Waits for job to complete.\n",
    "\n",
    "#Download file from google cloud storage to datalab compute instance, prefix headers with 'ga:' and upload to google analytics via data import api.\n",
    "def cs_to_ga(bucket_name, source_blob_name, destination_file_name, account_id, web_property_id, custom_data_source_id, project_id, prefix=True):\n",
    "    #Download file from cloud storage\n",
    "    storage_client = StorageClient(project=project_id)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    #prefix column headers by reding to pandas and then back to csv-file            \n",
    "    df = read_csv(destination_file_name)\n",
    "    if prefix:\n",
    "        df = df.add_prefix('ga:')\n",
    "    df.to_csv(destination_file_name, index=False)\n",
    "\n",
    "    #upload to GA data import\n",
    "    media = MediaFileUpload(destination_file_name, mimetype='application/octet-stream', chunksize=1024*1024, resumable=True)\n",
    "    try:\n",
    "        daily_upload = analytics.management().uploads().uploadData(accountId=account_id, webPropertyId=web_property_id, customDataSourceId=custom_data_source_id, media_body=media).execute()\n",
    "    except TypeError, error:\n",
    "        # Handle errors in constructing a query.\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise\n",
    "        \n",
    "    #Delete file from local when finished\n",
    "    os.remove(destination_file_name)\n",
    "\n",
    "def main(query, project_id, bucket, blob, account_id, web_property_id, data_source_id):\n",
    "    bq_to_cs(query, project_id, blob, bucket)\n",
    "    cs_to_ga(bucket, blob, blob, account_id, web_property_id, data_source_id, project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "query = '''SELECT '/' AS pagePath, 'Author A' AS dimension1\n",
    "UNION ALL\n",
    "SELECT '/flatten-google-analytics-custom-dimensions-with-a-bigquery-udf/' AS pagePath, 'Author B' AS dimension1'''\n",
    "\n",
    "main(query , \"<GCP project>\", \"<GCS bucket>\", \"export.csv\", \"<GA account>\", \"<GA property>\", \"<GA Data Set ID>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enable google analytics api on project\n",
    "restart interpreter after installing requests>=2.18.0 to make sure that import doesn't check if module is in sys.module and returning old version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
